@misc{ASY19,
	author={Takuya Akiba and Shotaro Sano and Toshihiko Yanase and Takeru Ohta and Masanori Koyama},
	title={Optuna: A Next-generation Hyperparameter Optimization Framework}, 
	year={2019},
	eprint={1907.10902},
	archivePrefix={arXiv},
	primaryClass={cs.LG},
	url={https://arxiv.org/abs/1907.10902}, 
}

@article{ANTS23,
    author = {Ahmed, S. and Nielsen, I.E. and Tripathi, A. and Siddiqui, S. and Ramachandran R. P. and  Rasool G.}, 
    title = {Transformers in Time-Series Analysis: A Tutorial}, 
    journal = {Circuits Syst Signal Processing},
    volume = 42, 
    pages = {7433–7466}, 
    year = 2023 
    }

@book{BJ70,
  title={Time Series Analysis: Forecasting and Control},
  author={Box, G.E.P. and Jenkins, G.M.},
  isbn={9780816210947},
  lccn={lc77079534},
  series={Holden-Day series in time series analysis and digital processing},
  url={https://books.google.it/books?id=5BVfnXaq03oC},
  year={1970},
  publisher={Holden-Day}
}

@misc{CGCB14,
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  biburl = {https://www.bibsonomy.org/bibtex/24e484a5052abe2440c0e02fd1490adfa/izzy278},
  note = {cite arxiv:1412.3555Comment: Presented in NIPS 2014 Deep Learning and Representation Learning  Workshop},
  title = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
  Modeling},
  url = {http://arxiv.org/abs/1412.3555},
  year = 2014
}

@misc{G25,
  author = {Garagnani, F.},
  year   = {2025},
  title  = {deconstructing-transformers},
  note   = {\url{https://github.com/FGaragnani/deconstructing-transformers}, 
            last accessed in 08.08.2025},
  urldate = {01.06.2025}
}

@article{GEW06,
  author = {P. Geurts and D. Ernst and L. Wehenkel}, 
  title = {Extremely randomized trees}, 
  journal = {Machine Learning},
  volume = 63,
  number = 1, 
  pages = {3-42}, 
  year = 2006
}

@misc{googletrends2025,
  author = {{Google}},
  title = {Google Trends},
  howpublished = {\url{https://trends.google.com}},
  note = {Accessed: 2025-08-27},
  year = {2025}
}

@article{HS97,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
journal = {Neural Computation},
pages = {1735–1780}
}

@article{H57,
  title={Forecasting trends and seasonals by exponentially weighted moving averages},
  author={Holt, Charles C},
  journal={ONR Memorandum},
  volume={52},
  number={52},
  pages={5--10},
  year={1957},
  publisher={Carnegie Institute of Technology}
}

@INPROCEEDINGS{LBB97,
  author={Yann Le Cun and Bottou, L. and Bengio, Y.},
  booktitle={1997 IEEE International Conference on Acoustics, Speech, and Signal Processing}, 
  title={Reading checks with multilayer graph transformer networks}, 
  year={1997},
  volume={1},
  number={},
  pages={151-154 vol.1},
}

@INPROCEEDINGS{LKR21,
	author={Lin, Yang and Koprinska, Irena and Rana, Mashud},
	booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
	title={Temporal Convolutional Attention Neural Networks for Time Series Forecasting}, 
	year={2021},
	volume={},
	number={},
	pages={1-8}
}
	
@inproceedings{LHZW24,
    title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},
    author={Yong Liu and Tengge Hu and Haoran Zhang and Haixu Wu and Shiyu Wang and Lintao Ma and Mingsheng Long},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=JePfAI8fah}
}

@article{LSNP21,
    author = {Lim, B and Sercan, O.A. and Loeff, N and Pfister, T},
    title = {Temporal Fusion Transformers for interpretable multi-horizon time series forecasting},
    journal = {International Journal of Forecasting},
    volume = {37},
    number = {4},
    pages = {1748-1764},
    year = {2021},
}

@article{MH00,
	author = {Spyros Makridakis and Michèle Hibon},
	title = {The M3-Competition: results, conclusions and implications},
	journal = {International Journal of Forecasting},
	volume = {16},
	number = {4},
	pages = {451-476},
	year = {2000},
	note = {The M3- Competition},
}

@misc{PyTorch25,
  year   = {2025},
  title  = {PyTorch Transformer},
  note   = {\url{https://docs.pytorch.org/docs/stable/generated/torch.nn.Transformer.html}, last accessed in 08.08.2025},
  urldate = {01.06.2025}
}

@inproceedings{SP10,
	author={Seabold, Skipper and Perktold, Josef},
	title={statsmodels: Econometric and statistical modeling with python},
	booktitle={9th Python in Science Conference},
	year={2010},
}

@MISC {S17,
	author = {Taylor G. Smith and others},
	title  = {{pmdarima}: ARIMA estimators for {Python}},
	year   = {2017--},
	url    = "http://www.alkaline-ml.com/pmdarima",
	note   = {[Online; accessed Sept 29, 2025]}
}

@article{SZLW25,
    author={Su, Liyilei and Zuo, Xumin and Li, Rui and Wang, Xin and Zhao, Heng and Huang, Bingding},
    year = 2025,
    title = {A systematic review for transformer-based long-term series forecasting},
    journal = {Artificial Intelligence Review},
    volume = 58,
    issue = 3,
    pages = {1573-7462}
}

@inproceedings{VSPU17,
     author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {Attention is All you Need},
    volume = {30},
     year = {2017}
}

@inproceedings{WZZC23,
    author = {Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
    title = {Transformers in time series: a survey},
    year = {2023},
    articleno = {759},
    booktitle = {Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
    numpages = {9},
    location = {Macao, P.R.China},
    series = {IJCAI '23}
}

@article{W60,
 author = {Peter R. Winters},
 journal = {Management Science},
 number = {3},
 pages = {324--342},
 publisher = {INFORMS},
 title = {Forecasting Sales by Exponentially Weighted Moving Averages},
 urldate = {2025-08-06},
 volume = {6},
 year = {1960}
}

@misc{WXWL22,
      title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}, 
      author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2106.13008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.13008}, 
}

@InProceedings{ZMWW22,
  title = 	 {{FED}former: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting},
  author =       {Zhou, Tian and Ma, Ziqing and Wen, Qingsong and Wang, Xue and Sun, Liang and Jin, Rong},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {27268--27286},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
}

@misc{ZZPZ21,
      title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting}, 
      author={Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
      year={2021},
      eprint={2012.07436},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2012.07436}, 
}

@misc{patchTST,
      title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers}, 
      author={Yuqi Nie and Nam H. Nguyen and Phanwadee Sinthong and Jayant Kalagnanam},
      year={2023},
      eprint={2211.14730},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2211.14730}, 
}

@article{chronos,
	title={Chronos: Learning the Language of Time Series},
	author={Ansari, Abdul Fatir and Stella, Lorenzo and Turkmen, Caner and Zhang, Xiyuan and Mercado, Pedro and Shen, Huibin and Shchur, Oleksandr and Rangapuram, Syama Syndar and Pineda Arango, Sebastian and Kapoor, Shubham and Zschiegner, Jasper and Maddix, Danielle C. and Mahoney, Michael W. and Torkkola, Kari and Gordon Wilson, Andrew and Bohlke-Schneider, Michael and Wang, Yuyang},
	journal={Transactions on Machine Learning Research},
	issn={2835-8856},
	year={2024},
	url={https://openreview.net/forum?id=gerNCVqqtR}
}

@inproceedings{tcan,
	author    = {Yang Lin and Irena Koprinska and Mashud Rana},
	title     = {Temporal Convolutional Attention Neural Networks for Time Series Forecasting},
	year      = {2021},
	booktitle = {Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN)},
}